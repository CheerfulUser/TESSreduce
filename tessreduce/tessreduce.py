"""
Import packages!
"""
import traceback
import os
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

import lightkurve as lk
from photutils.detection import StarFinder
from PRF import TESS_PRF

from copy import deepcopy

from scipy.ndimage import convolve
from scipy.ndimage import shift

from scipy.signal import savgol_filter

from scipy.interpolate import interp1d

from astropy.stats import sigma_clipped_stats
from astropy.stats import sigma_clip
from astropy.coordinates import SkyCoord
from astropy import units as u
from astropy.time import Time

import multiprocessing
from joblib import Parallel, delayed
from tqdm import tqdm

from .catalog_tools import *
from .calibration_tools import *
from .ground_tools import ground
from .rescale_straps import correct_straps
from .lastpercent import *
from .psf_photom import create_psf
from .helpers import *
from .cat_mask import Cat_mask

# turn off runtime warnings (lots from logic on nans)
import warnings
# nuke warnings because sigma clip is extremely annoying 
warnings.filterwarnings("ignore")
warnings.filterwarnings("ignore", category=RuntimeWarning) 
warnings.filterwarnings("ignore", category=np.VisibleDeprecationWarning)
pd.options.mode.chained_assignment = None
with warnings.catch_warnings():
	warnings.simplefilter("ignore")
	sigma_clip
	sigma_clipped_stats

# set the package directory so we can load in a file later
package_directory = os.path.dirname(os.path.abspath(__file__)) + '/'

fig_width_pt = 240.0  # Get this from LaTeX using \showthe\columnwidth
inches_per_pt = 1.0/72.27			   # Convert pt to inches
golden_mean = (np.sqrt(5)-1.0)/2.0		 # Aesthetic ratio
fig_width = fig_width_pt*inches_per_pt  # width in inches

class tessreduce():

	def __init__(self,ra=None,dec=None,name=None,obs_list=None,tpf=None,size=90,sector=None,
				 reduce=True,align=True,diff=True,corr_correction=True,calibrate=True,sourcehunt=True,
				 phot_method='aperture',imaging=False,parallel=True,num_cores=-1,diagnostic_plot=False,plot=True,
				 savename=None,quality_bitmask='default',cache_dir=None,catalogue_path=False,
				 prf_path=None,verbose=1):

		"""
		Class for extracting reduced TESS photometry around a target coordinate or event. 

		Parameters
		----------
		ra : float
			Right ascension of the target object. The default is None.
		dec : float
			Declination of the target object. The default is None.
		name : str
			Name of the object, used in saving. The default is None.
		obs_list : array_like, optional
			Array generated by the sn_lookup and spacetime_lookup functions. The default is None.
		tpf : target pixel file, optional
			TESS target pixel file. The default is None.
		size : int, optional
			Size in pixels of the cutout, larger cutout sizes give better background subtractions. The default is 90.
		sector : int, optional
			Sector of observations. The default is None.
		phot_method : str, optional
			Select the photometry method used in the reduction. Choose between 'aperture' and 'PSF'. The default is 'aperture'.

		Options
		-------
		reduce : bool, optional
			Perform photometric reduction processes for the target region. The default is True.
		align : bool, optional
			Shift images to align stars with a reference frame. The default is True.
		diff : bool, optional
			Calculate difference imaging between each frame and a reference frame. The default is True.
		corr_correction : bool, optional
			Final correction step that operates on pixels which have high correlation between their lightcurve and the background. The default is True.
		calibrate : bool, optional
			Performs photometric calibration on the datacube using PS1 or SkyMapper data. The default is True.
		sourcehunt : bool, optional
			Searches for sources in the background and masks them. Prevents asteroids and other transients from being included in the background. The default is True.
		imaging : bool, optional
			Retrieves PanSTARRS or SkyMapper photometry of the region. The default is False.
		parallel : bool, optional
			Perform computation with parallel processing using 'num_cores'. The default is True.
		num_cores : int, optional
			Number of cores to run parallel process on. The default is -1 which uses max system cores.
		diagnostic_plot : bool, optional
			During reduction, plot figures which outline various calculation steps, such as the image shifts over time or the zeropoint calculation. The default is False.
		plot : bool, optional
			During reduction, plot resulting light curve. The default is True.
		savename : str, optional
			Save name for the outputs. The default is None.
		quality_bitmask : str, optional
			Parameter for lightkurve download of TESS TPF. The default is 'default'.
		cache_dir : str, optional
			Directory to cache files. The default is None.
		catalogue_path : str, optional
			Path to required catalogs for when using TESSreduce in offline mode. The default is False.
		prf_path : str, optional
			Path to local TESS PRF files. The default is currently a specific location on the OzStar supercomputer.
		verbose : int, optional
			Controls the level of verbosity, 0 is none, 1 is verbose. The default is 1.

		"""

		# Field Specific
		self.ra = ra
		self.dec = dec 
		self.name = name
		self.size = size
		self.sector = sector
		self.tpf = tpf

		# Reduction Process Specific
		self.align = align
		self.calibrate = calibrate
		self.corr_correction = corr_correction
		self.diff = diff
		self.imaging = imaging
		self.parallel = parallel
		if type(num_cores) == str:
			self.num_cores = multiprocessing.cpu_count()
		else:
			self.num_cores = num_cores
		self._assign_phot_method(phot_method)
		self._sourcehunt = sourcehunt
		self.verbose = verbose

		# Offline Paths 
		if catalogue_path is None:
			catalogue_path = os.getcwd()
		elif catalogue_path is False:
			catalogue_path = None
		self._catalogue_path = catalogue_path
		self.num_cores = num_cores
		self.imaging = imaging
		self._prf_path = prf_path

		# Plotting
		self.plot = plot
		self.diagnostic_plot = diagnostic_plot
		self.savename = savename

		# Calculated 
		self.mask = None
		self.shift = None
		self.bkg = None
		self.flux = None
		self.ref = None
		self.ref_ind = None
		self.wcs = None	
		self.qe = None
		self.lc = None
		self.sky = None
		self.events = None
		self.zp = None
		self.zp_e = None
		self.sn_name = None
		self.ebv = 0
		# repeat for backup
		self.tzp = None
		self.tzp_e = None
		
		# light curve units 
		self.lc_units = 'Counts'

		# Generate coordinate information from 'obs_list'
		if obs_list is not None:
			if isinstance(obs_list,list):
				obs_list = np.array(obs_list,dtype=object)
				if len(obs_list.shape) > 1:
					obs_list = obs_list[obs_list[:,3].astype('bool')][0]
				self.ra = obs_list[0]
				self.dec = obs_list[1]
				self.sector = obs_list[2]
			elif isinstance(obs_list, pd.DataFrame):
				self.ra = obs_list['RA'].to_numpy()[0]
				self.dec = obs_list['DEC'].to_numpy()[0]
				self.sector = obs_list['Sector'].to_numpy()
			if isinstance(obs_list,list):
				obs_list = np.array(obs_list,dtype=object)
				if len(obs_list.shape) > 1:
					obs_list = obs_list[obs_list[:,3].astype('bool')][0]
				self.ra = obs_list[0]
				self.dec = obs_list[1]
				self.sector = obs_list[2]
			elif isinstance(obs_list, pd.DataFrame):
				self.ra = obs_list['RA'].to_numpy()[0]
				self.dec = obs_list['DEC'].to_numpy()[0]
				self.sector = obs_list['Sector'].to_numpy()

		# Generate coordinate information from 'tpf'
		if tpf is not None:
			if type(tpf) == str:
				self.tpf = lk.TessTargetPixelFile(tpf)
			self.flux = strip_units(self.tpf.flux)
			self.wcs = self.tpf.wcs
			self.ra = self.tpf.ra
			self.dec = self.tpf.dec
			self.size = self.tpf.flux.shape[1]
			self.sector = self.tpf.sector
			if self.sector is None:
				self.sector = 30

		# Retrieve TPF
		elif self.check_coord():
			if self.verbose>0:
				print('getting TPF from TESScut')
			self.get_TESS(quality_bitmask=quality_bitmask,cache_dir=cache_dir)
			self._get_gaia()

		self.ground = ground(ra = self.ra, dec = self.dec)

		if reduce:
			self.reduce()

	def check_coord(self):
		"""
		Checks if target coordinate / name input is valid.

		Returns
		-------
		bool.

		"""

		if ((self.ra is None) | (self.dec is None)) & (self.name is None):
			return False
		else:
			return True

	def _get_gaia(self,maglim=21):
		"""
		Downloads a catalogue of stars in the target region from GAIA DR3.

		Parameters
		----------
		maglim : int,float
			Limiting magnitude in GAIA g band of stars to include in catalogue. The default is 21.
		
		Assigns
		-------
		gaia : DataFrame
			Catalogue of GAIA sources inside cutout region.

		"""
		
		# Get dataframe from Gaia around cutout
		result = Get_Catalogue(self.tpf, Catalog = 'gaia')
		result = result[result.Gmag < maglim]
		result = result.rename(columns={'RA_ICRS': 'ra',
							   'DE_ICRS': 'dec',
							   'e_RA_ICRS': 'e_ra',
							   'e_DE_ICRS': 'e_dec',})
		
		# Convert star RA/DEC to pixel values and input into dataframe
		x,y = self.wcs.all_world2pix(result['ra'].values,result['dec'].values,0)
		result['x'] = x; result['y'] = y

		# Restrict catalogue to only objects inside cutout
		ind = (((x > 0) & (y > 0)) & 
		 	  ((x < (self.flux.shape[2])) & (y < (self.flux.shape[1]))))
		result = result.iloc[ind]

		self.gaia = result

	def _assign_phot_method(self,phot_method):
		"""
		Assigns reduction photometry extraction method, currently aperture and PSF photometry is supported.

		Parameters
		----------
		phot_method : str
			Photometry extraction method, either 'aperture' or 'PSF'.

		Raises
		------
		ValueError
			If phot_method is not in the above options.

		Assigns
		-------
		phot_method : str
			Photometry extraction method.

		"""

		if type(phot_method) == str:
			method = phot_method.lower()
			if (method == 'psf') | (method == 'aperture'):
				self.phot_method = method
			else:
				m = f'The input method "{method}" is not supported, please select either "psf", or "aperture".'
				raise ValueError(m)
		else:
			m = 'phot_mehtod must be a string equal to either "psf", or "aperture".'
			raise ValueError(m)

	def get_TESS(self,ra=None,dec=None,name=None,size=None,sector=None,quality_bitmask='default',cache_dir=None):
		"""
		Use the lightcurve interface with TESScut to get an FFI cutout 
		of a region around the given coords.

		Parameters
		----------
		ra : float, optional
			RA of the cutout centre. The default is None.
		dec : float, optional
			Dec of the cutout centre. The default is None.
		name : str, optional
			Name of target event in TNF. The default is None.
		size : int, optional
			Size of the cutout (size x size). The default is None.
		sector : int, optional
			Sector to download. The default is None.
		quality_bitmask : str, optional
		 	Parameter for Lightkurve download of TESS TPF. The default is 'default'.
		cache_dir : str, optional
			Directory to cache files. The default is None.

		Raises
		------
		ValueError
			If download process fails for whatever reason.

		Assigns
		-------
		tpf : Lightkurve Target Pixel File
			Tess FFI cutout of the selected region.
		flux : np.array
			Array of flux values stored in tpf.
		wcs : astropy WCS object
			World coordinate system information of the cutout.

		"""
		
		if sector is None:
			sector = self.sector

		# Find download file from Lightkurve based on coordinate / name
		if (name is None) & (self.name is None):
			if (ra is not None) & (dec is not None):
				c = SkyCoord(ra=float(ra)*u.degree, dec=float(dec) *
								u.degree, frame='icrs')
			else:
				c = SkyCoord(ra=float(self.ra)*u.degree, dec=float(self.dec) *
								u.degree, frame='icrs')
			tess = lk.search_tesscut(c,sector=sector)
		else:
			tess = lk.search_tesscut(name,sector=sector)
		if size is None:
			size = self.size

		# Download 
		tpf = tess.download(quality_bitmask=quality_bitmask,cutout_size=size,download_dir=cache_dir)

		# Check to ensure it succeeded
		if tpf is None:
			m = 'Failure in TESScut api, not sure why.'
			raise ValueError(m)
		
		self.tpf  = tpf
		self.flux = strip_units(tpf.flux)  # Stripping astropy units so only numbers are returned
		self.wcs  = tpf.wcs

	def make_mask(self,catalogue_path=None,maglim=19,scale=1,strapsize=6,useref=False):
		"""
		Generate a source mask for the cutout region from source catalogues.
		Pixels that are found to include sources will not be included in background calculation.

		Parameters
		----------
		catalogue_path : str, optional
			Local path to source catalogue if using TESSreduce in offline mode. The default is None.
		maglim : float, optional
			Limiting magnitude of sources to include in the source mask. The default is 19.
		scale : float, optional
			Adjusts how much of each source the mask covers. The default is 1.
		strapsize : float, optional
			Width in pixels of the mask for TESS' electrical straps. The default is 6.
		
		Options
		-------
		useref : bool, optional
			Generate the mask solely from the reference frame. The default is False.

		Assigns
		-------
		mask : np.array
			A bitwise source mask for the cutout. Bits are as follows:
				0 - background
				1 - catalogue source
				2 - saturated source
				4 - strap mask
				8 - bad pixel (not used)

		"""

		data = strip_units(self.flux)

		# Generate mask from source catalogue
		if useref:
			mask, cat = Cat_mask(self.tpf,catalogue_path,maglim,scale,strapsize,ref=self.ref)
		else:
			mask, cat = Cat_mask(self.tpf,catalogue_path,maglim,scale,strapsize)

		# Generate sky background as the inverse of mask
		sky = ((mask & 1)+1 ==1) * 1.
		sky[sky==0] = np.nan
		tmp = np.nansum(data*sky,axis=(1,2))
		tmp[tmp==0] = 1e12 # random big number 
		ref = data[np.argmin(tmp)] * sky

		# Correct for the electrical straps
		try:
			qe = correct_straps(ref,mask,parallel=True)
		except:
			qe = correct_straps(ref,mask,parallel=False)
		

		c1 = data.shape[1] // 2
		c2 = data.shape[2] // 2
		cmask = np.zeros_like(data[0],dtype=int)
		cmask[c1,c2] = 1
		kern = np.ones((5,5))
		cmask = convolve(cmask,kern)

		fullmask = mask | cmask
		sky = ((fullmask & 1)+1 ==1) * 1.
		sky[sky==0] = np.nan
		masked = ref*sky
		mean,med,std = sigma_clipped_stats(masked)# assume sources weight the mean above the bkg
		if useref is False:
			m_second = (masked > mean+2*std).astype(int)
			self.mask = fullmask | m_second
		else:
			self.mask = fullmask
		self._mask_cat = cat

	def psf_source_mask(self,sigma=5):
		"""
		Generate a source mask by finding PSF like objects in each image.

		Parameters
		----------
		sigma : float, optional
			Photometric spread of source considered for finding sources. The default is 5.

		Assigns
		-------
		prf : TESS PRF Object
			Pixel response function object for this cutout.

		Returns 
		-------
		m : np.array
			Source bitmask.
			
		"""

		# Find PRF for cutout (depends on Sector, Camera, CCD, Pixel Row, Pixel Column)
		if self._catalogue_path is not None:

			if self.sector < 4:
				prf = TESS_PRF(self.tpf.camera,self.tpf.ccd,self.sector,
								self.tpf.column+self.flux.shape[2]/2,self.tpf.row+self.flux.shape[1]/2,
								localdatadir=f'{self._prf_path}/Sectors1_2_3')
			else:
				prf = TESS_PRF(self.tpf.camera,self.tpf.ccd,self.sector,
								self.tpf.column+self.flux.shape[2]/2,self.tpf.row+self.flux.shape[1]/2,
								localdatadir=f'{self._prf_path}/Sectors4+')
		else:
			prf = TESS_PRF(self.tpf.camera,self.tpf.ccd,self.sector,
				   	   		self.tpf.column+self.flux.shape[2]/2,self.tpf.row+self.flux.shape[1]/2)
		
		self.prf =  prf.locate(5,5,(11,11))

		# Iterate through frames to find PRF like sources
		data = (self._flux_aligned - self.ref) #* mask
		if self.parallel:
			try:
				m = Parallel(n_jobs=self.num_cores)(delayed(par_psf_source_mask)(frame,self.prf,sigma) for frame in data)
				m = np.array(m)
			except:
				m = np.ones_like(data)
				for i in range(data.shape[0]):
					#m[i] = _par_psf_source_mask(data[i],self.prf,sigma)
					eh = par_psf_source_mask(data[i],self.prf,sigma)
					m[i] = eh
		else:
			m = np.ones_like(data)
			for i in range(data.shape[0]):
				m[i] = par_psf_source_mask(data[i],self.prf,sigma)
		return m * 1.0

	def background(self,gauss_smooth=2,calc_qe=True, strap_iso=True,source_hunt=False,interpolate=True):
		"""
		Calculate the temporal and spatial variation in the background.

		Parameters
		----------
		gauss_smooth : float, optional
			Smoothing factor for the background smoothing. The default is 2.

		Options
		-------
		calc_qe : bool, optional
			Calculate the quantum efficiency in TESS' electrical straps. The default is True.
		strap_iso : bool, optional
			Isolate the electrical straps for calculation. The default is True.
		source_hunt : bool, optional
			Using PSF, search for sources in each frame that may not have been masked out by the catalogue source mask. The default is False.
		interpolate : bool, optional
			Interpolate over masked out objects when calculating background. The default is True.
		
		Assigns
		-------
		bkg : np.array
			Spatially varying background for each frame.

		"""

		if strap_iso:
			m = (self.mask == 0) * 1.
		else:
			m = ((self.mask & 1 == 0) & (self.mask & 2 == 0) ) * 1.
		m[m==0] = np.nan

		# Find extra sources not found in catalogue mask
		if source_hunt:
			sm = self.psf_source_mask()
			sm[sm==0] = np.nan
			m = sm * m
		self._bkgmask = m

		# Calculate the smooth background 
		if (self.flux.shape[1] > 30) & (self.flux.shape[2] > 30):
			flux = strip_units(self.flux)

			bkg_smth = np.zeros_like(flux) * np.nan
			if self.parallel:
				bkg_smth = Parallel(n_jobs=self.num_cores)(delayed(Smooth_bkg)(frame,gauss_smooth,interpolate) for frame in flux*m)
			else:
				for i in range(flux.shape[0]):
					bkg_smth[i] = Smooth_bkg((flux*m)[i],gauss_smooth,interpolate)
		else:
			print('Small tpf, using percentile cut background')
			self.small_background()
			bkg_smth = self.bkg
		
		# Calculate quantum efficiency 
		if calc_qe:
			strap = (self.mask == 4) * 1.0
			strap[strap==0] = np.nan
			# check if its a time varying mask
			if len(strap.shape) == 3: 
				strap = strap[self.ref_ind]
			mask = ((self.mask & 1) == 0) * 1.0
			mask[mask==0] = np.nan
		
			data = strip_units(self.flux) * mask
			norm = self.flux / bkg_smth
			straps = norm * ((self.mask & 4)>0)
			limit = np.nanpercentile(straps,60,axis=1)
			straps[limit[:,np.newaxis,:] < straps] = np.nan
			straps[straps==0] = 1

			value = np.nanmedian(straps,axis=1)
			qe = np.ones_like(bkg_smth) * value[:,np.newaxis,:]
			bkg = bkg_smth * qe
			self.qe = qe
		else:
			bkg = np.array(bkg_smth)
		self.bkg = bkg

	def small_background(self):
		"""
		A different background calculation for if the cutout is too small for high quality results using default process.

		Assigns
		-------
		bkg : np.array
			Spatially varying background for each frame.

		"""
		
		bkg = np.zeros_like(self.flux)
		flux = strip_units(self.flux)
		lim = 2*np.nanmin(flux,axis=(1,2)) #np.nanpercentile(flux,1,axis=(1,2))
		ind = flux > lim[:,np.newaxis,np.newaxis]
		flux[ind] = np.nan
		val = np.nanmedian(flux,axis=(1,2))
		bkg[:,:,:] = val[:,np.newaxis,np.newaxis]
		self.bkg = bkg

	def _bkg_round_3(self,iters=5):
		"""
		Third background calculation.

		Parameters
		----------
		iters : int, optional
			Number of iterations to clean background. The default is 5.

		Assigns
		-------
		bkg : np.array
			Spatially varying background for each frame.

		"""
		
		for i in range(iters):
			tb = self.bkg * self._bkgmask
			m = np.nanmedian(tb,axis=(1,2))
			std = np.nanstd(tb,axis=(1,2))
			sbkg = np.nansum(self.bkg,axis=(1,2))
			ind = sbkg > np.nanpercentile(sbkg,95)

			frame,y,x = np.where((self.bkg>(2*std+m)[:,np.newaxis,np.newaxis]) | (self.bkg<(m - 2*std)[:,np.newaxis,np.newaxis]))

			dist_mask = np.zeros_like(self.flux)

			dist_mask[frame,y,x] = 1
			dist_mask[ind] = 0 # reset the bright frames since they are unreliable 
			common = np.sum(dist_mask,axis=0) > len(dist_mask) * 0.3
			dist_mask[:,common] = 1
			kern = np.ones((1,3,3))
			dist_mask = convolve(dist_mask,kern) > 0
			if self.parallel:
				bkg_3 = Parallel(n_jobs=self.num_cores)(delayed(parallel_bkg3)(self.bkg[i],dist_mask[i]) 
														   for i in np.arange(len(dist_mask)))
			else:
				bkg_3 = []
				bkg_smth = np.zeros_like(dist_mask)
				for i in range(len(dist_mask)):
					bkg_3[i] = parallel_bkg3(self.bkg[i],dist_mask[i])
			self.bkg = np.array(bkg_3)

	def _clip_background(self,sigma=5,ideal_size=90):
		"""
		Performs sigma clip on the background and recomputes clipped points.

		Parameters
		----------
		sigma : float, optional
			Number of sigma to cut background. The default is 5.

		Returns
		-------
		None

		"""
		
		if self.parallel:
			bkg_clip = Parallel(n_jobs=self.num_cores)(delayed(clip_background)(self.bkg[i],self.mask,sigma,ideal_size) 
													   for i in np.arange(len(self.bkg)))
		else:
			bkg_clip = []
			for i in range(len(self.bkg)):
				bkg_clip[i] = clip_background(self.bkg[i],self.mask,ideal_size)
		self.bkg = np.array(bkg_clip)

	def _grad_bkg_clip(self,sigma=3,max_size=1000):
		"""
		Performs sigma clip on the background based on gradients and recomputes clipped points.

		Parameters
		----------
		sigma : float, optional
			Number of sigma to cut background. The default is 3.
		max_size: int, optional
			Maximum allowable size of a region to be clipped.

		Assigns
		-------
		bkg : np.array
			Assigns the recomputed tess background.
		"""
		
		if self.parallel:
			bkg_clip = Parallel(n_jobs=self.num_cores)(delayed(grad_clip_fill_bkg)(self.bkg[i],sigma,max_size) 
													   for i in np.arange(len(self.bkg)))
		else:
			bkg_clip = []
			for i in range(len(self.bkg)):
				bkg_clip[i] = grad_clip_fill_bkg(self.bkg[i],max_size)
		self.bkg = np.array(bkg_clip)


	def get_ref(self,start = None, stop = None):
		"""
		Get reference image to use for subtraction and mask creation.
		The image is made from all images with low background light.

		Parameters
		----------
		start : int, optional
			First frame to consider for reference determination. The default is None.
		stop : int, optional
			Final frame to consider for reference determination. The default is None.

		Assigns
		-------
		ref : np.array
			Reference image for the cutout.
		ref_ind : int
			Index pointing to which frame was used for reference.

		"""

		data = strip_units(self.flux)
		if (start is None) & (stop is None):
			start = 0
			stop = len(self.flux)
		elif (start is not None) & (stop is None):
			stop = len(self.flux)

		elif (start is None) & (stop is not None):
			start = 0

		start = int(start)
		stop = int(stop)

		ind = self.tpf.quality[start:stop] == 0
		d = deepcopy(data[start:stop])[ind]
		summed = np.nansum(d,axis=(1,2))
		lim = np.percentile(summed[np.isfinite(summed)],5)
		summed[summed>lim] = 0
		inds = np.where(ind)[0]
		ref_ind = start + inds[np.argmax(summed)]
		reference = data[ref_ind]
		if len(reference.shape) > 2:
			reference = reference[0]
			ref_ind = ref_ind[0]
		
		self.ref = reference
		self.ref_ind = ref_ind

	def centroids_shifts_starfind(self,plot=None,savename=None):
		"""
		Depricated.
		Calculate the centroid shifts of sources for time series images using Starfinding based on TESS PRF.

		Options
		----------
		plot : bool, optional
			Plot a diagnostic figure for the shift calculation. The default is None.
		savename : str, optional
			Save name for output. The default is None.

		Assigns
		-------
		shift : np.array
			Median x,y shift of sources over the time series.

		"""

		if plot is None:
			plot = self.diagnostic_plot
		if savename is None:
			savename = self.savename

		# hack solution for new lightkurve
		f = strip_units(self.flux)
		m = self.ref.copy()

		mean, med, std = sigma_clipped_stats(m, sigma=3.0)

		prf = TESS_PRF(self.tpf.camera,self.tpf.ccd,self.tpf.sector,
				   	   self.tpf.column+self.flux.shape[2]/2,self.tpf.row+self.flux.shape[1]/2)
		self.prf =  prf.locate(5,5,(11,11))
		
		finder = StarFinder(2*std,kernel=self.prf,exclude_border=True)
		s = finder.find_stars(m-med)
		
		mx = s['xcentroid']
		my = s['ycentroid']
		x_mid = self.flux.shape[2] / 2
		y_mid = self.flux.shape[1] / 2
		
		self._dat_sources = s.to_pandas()
		
		if self.parallel:
			shifts = Parallel(n_jobs=self.num_cores)(
				delayed(Calculate_shifts)(frame,mx,my,finder) for frame in f)
			shifts = np.array(shifts)
		else:
			shifts = np.zeros((len(f),2,len(mx))) * np.nan
			for i in range(len(f)):
				shifts[i,:,:] = Calculate_shifts(f[i],mx,my,finder)

		self.raw_shifts = shifts
		meds = np.nanmedian(shifts,axis = 2)
		meds[~np.isfinite(meds)] = 0

		smooth = Smooth_motion(meds,self.tpf)
		nans = np.nansum(f,axis=(1,2)) ==0
		smooth[nans] = np.nan
		self.shift = meds #smooth
		
		if plot:
			t = self.tpf.time.mjd
			ind = np.where(np.diff(t) > .5)[0]
			smooth[ind,:] = np.nan
			plt.figure(figsize=(1.5*fig_width,1*fig_width))
			plt.plot(t,meds[:,1],'.',label='Row shift',alpha =0.5)
			plt.plot(t,smooth[:,1],'-',label='Smoothed row shift')
			plt.plot(t,meds[:,0],'.',label='Col shift',alpha =0.5)
			plt.plot(t,smooth[:,0],'-',label='Smoothed col shift')
			plt.ylabel('Shift (pixels)',fontsize=15)
			plt.xlabel('Time (MJD)',fontsize=15)
			plt.legend()
			plt.show()
			if savename is not None:
				plt.savefig(savename+'_disp.pdf', bbox_inches = "tight")
		
	def fit_shift(self,plot=None,savename=None):
		"""
		Calculate the centroid shifts of sources for time series images 
		by finding the shifts which minimize the difference between frames and reference.

		Options
		----------
		plot : bool, optional
			Plot a diagnostic figure for the shift calculation. The default is None.
		savename : str, optional
			Save name for output. The default is None.

		Assigns
		-------
		shift : np.array
			Median x,y shift of sources over the time series.

		"""

		if plot is None:
			plot = self.diagnostic_plot
		if savename is None:
			savename = self.savename
		
		sources = ((self.mask & 1) ==1) * 1.0 - (self.mask & 2)
		sources[sources<=0] = 0

		f = self.flux 
		m = self.ref.copy() * sources
		m[m==0] = np.nan

		if self.parallel:
			shifts = Parallel(n_jobs=self.num_cores)(
				delayed(difference_shifts)(frame,m) for frame in f)
			shifts = np.array(shifts)
		else:
			shifts = np.zeros((len(f),2)) * np.nan
			for i in range(len(f)):
				shifts[i,:] = difference_shifts(f[i],m)

		if self.shift is not None:
			self.shift += shifts
		else:
			self.shift = shifts

		if plot:
			t = self.tpf.time.mjd
			ind = np.where(np.diff(t) > .5)[0]
			shifts[ind,:] = np.nan
			plt.figure(figsize=(1.5*fig_width,1*fig_width))
			plt.plot(t,shifts[:,0],'.',label='Row shift',alpha =0.5)
			plt.plot(t,shifts[:,1],'.',label='Col shift',alpha =0.5)
			plt.ylabel('Shift (pixels)',fontsize=15)
			plt.xlabel('Time (MJD)',fontsize=15)
			plt.legend()
			plt.show()
			if savename is not None:
				plt.savefig(savename+'_disp_corr.pdf', bbox_inches = "tight")

	def shift_images(self,median=False):
		"""
		Shifts each target image to the reference using the values given in offset. Breaks horribly if data is all 0.

		Options
		----------
		median : bool, optional
			Shift the reference to the target images using the reverse of the shifts. The default is False.

		Assigns
		-------
		flux : np.array
			Array of flux values now shifted to be best alignment.

		"""

		shifted = self.flux.copy()
		nans = ~np.isfinite(shifted)
		shifted[nans] = 0.
		if median:
			for i in range(len(shifted)):
				if np.nansum(abs(shifted[i])) > 0:
					shifted[i] = shift(self.ref,[-self.shift[i,1],-self.shift[i,0]])
			self.flux -= shifted

		else:
			for i in range(len(shifted)):
				if np.nansum(abs(shifted[i])) > 0:
					shifted[i] = shift(shifted[i],[self.shift[i,0],self.shift[i,1]],mode='nearest',order=5)#mode='constant',cval=np.nan)
			self.flux = shifted
		
	def bin_data(self,lc=None,time_bin=6/24,frames = None):
		"""
		Bin a light curve to the desired duration specified by bin_size

		Parameters
		----------
		lc : TYPE, optional
			DESCRIPTION. The default is None.
		time_bin : TYPE, optional
			DESCRIPTION. The default is 6/24.
		frames : TYPE, optional
			DESCRIPTION. The default is None.

		Returns
		-------
		binlc : TYPE
			DESCRIPTION.

		"""

		if lc is None:
			lc = self.lc
		else:
			if lc.shape[0] > lc.shape[1]:
				lc = lc.T
		flux = lc[1]
		try:
			err = lc[2]
		except:
			err = deepcopy(lc[1]) * np.nan
		t	= lc[0]
		if time_bin is None:
			bin_size = int(frames)
			lc = []
			x = []
			for i in range(int(len(flux)/bin_size)):
				if np.isnan(flux[i*bin_size:(i*bin_size)+bin_size]).all():
					lc.append(np.nan)
					x.append(int(i*bin_size+(bin_size/2)))
				else:
					lc.append(np.nanmedian(flux[i*bin_size:(i*bin_size)+bin_size]))
					x.append(int(i*bin_size+(bin_size/2)))
			binlc = np.array([t[x],lc])
		else:
			
			points = np.arange(t[0]+time_bin*.5,t[-1],time_bin)
			time_inds = abs(points[:,np.newaxis] - t[np.newaxis,:]) <= time_bin/2
			l = []
			e = []
			for i in range(len(points)):
				l += [np.nanmedian(flux[time_inds[i]])]
				e += [np.nanmedian(err[time_inds[i]])]
			l = np.array(l)
			e = np.array(e)
			binlc = np.array([points,l,e])
		return binlc


	def bin_flux(self,flux=None,time_bin=6/24,frames = None):
		"""
		Bin a light curve flux to the desired duration specified by bin_size

		Parameters
		----------
		flux : TYPE, optional
			DESCRIPTION. The default is None.
		time_bin : TYPE, optional
			DESCRIPTION. The default is 6/24.
		frames : TYPE, optional
			DESCRIPTION. The default is None.

		Returns
		-------
		binf : TYPE
			DESCRIPTION.
		bint : TYPE
			DESCRIPTION.

		"""

		if flux is None:
			flux = self.flux

		t = self.tpf.time.mjd

		if time_bin is None:
			bin_size = int(frames)
			f = []
			x = []
			for i in range(int(len(flux)/bin_size)):
				if np.isnan(flux[i*bin_size:(i*bin_size)+bin_size]).all():
					f.append(np.nan)
					x.append(int(i*bin_size+(bin_size/2)))
				else:
					f.append(np.nanmedian(flux[i*bin_size:(i*bin_size)+bin_size],axis=0))
					x.append(int(i*bin_size+(bin_size/2)))
			binf = np.array(f)
			bint = np.array(x)
		else:
			
			points = np.arange(t[0]+time_bin*.5,t[-1],time_bin)
			time_inds = abs(points[:,np.newaxis] - t[np.newaxis,:]) <= time_bin/2
			f = []
			for i in range(len(points)):
				f += [np.nanmedian(flux[time_inds[i]],axis=0)]
			binf = np.array(f)
			bint = np.array(points)
		return binf, bint

	def diff_lc(self,time=None,x=None,y=None,ra=None,dec=None,tar_ap=3,
				sky_in=5,sky_out=9,phot_method=None,psf_snap='brightest',plot=None,savename=None,mask=None,diff = True):
		"""
		Calculate the difference imaged light curve. if no position is given (x,y or ra,dec)
		then it degaults to the centre. Sky flux is calculated with an annulus aperture surrounding 
		the target aperture and subtracted from the source. The sky aperture undergoes sigma clipping
		to remove pixels that are poorly subtracted and contain other sources.

		Parameters
		----------
		time : TYPE, optional
			DESCRIPTION. The default is None.
		x : TYPE, optional
			DESCRIPTION. The default is None.
		y : TYPE, optional
			DESCRIPTION. The default is None.
		ra : TYPE, optional
			DESCRIPTION. The default is None.
		dec : TYPE, optional
			DESCRIPTION. The default is None.
		tar_ap : TYPE, optional
			DESCRIPTION. The default is 3.
		sky_in : TYPE, optional
			DESCRIPTION. The default is 5.
		sky_out : TYPE, optional
			DESCRIPTION. The default is 9.
		phot_method : TYPE, optional
			DESCRIPTION. The default is None.
		plot : TYPE, optional
			DESCRIPTION. The default is None.
		savename : TYPE, optional
			DESCRIPTION. The default is None.
		mask : TYPE, optional
			DESCRIPTION. The default is None.
		diff : TYPE, optional
			DESCRIPTION. The default is True.

		Returns
		-------
		lc : TYPE
			DESCRIPTION.
		sky : TYPE
			DESCRIPTION.

		"""
		if plot is None:
			plot = self.diagnostic_plot
		if savename is None:
			savename = self.savename
		if phot_method is None:
			phot_method = self.phot_method

		data = strip_units(self.flux)
		if ((ra is None) | (dec is None)) & ((x is None) | (y is None)):
			ra = self.ra 
			dec = self.dec

		if tar_ap // 2 == tar_ap / 2:
			print(Warning('tar_ap must be odd, adding 1'))
			tar_ap += 1
		if sky_out // 2 == sky_out / 2:
			print(Warning('sky_out must be odd, adding 1'))
			sky_out += 1
		if sky_in // 2 == sky_in / 2:
			print(Warning('sky_out must be odd, adding 1'))
			sky_in += 1
			
		if (ra is not None) & (dec is not None) & (self.tpf is not None):
			x,y = self.wcs.all_world2pix(ra,dec,0)
			x = int(x + 0.5)
			y = int(y + 0.5)
		elif (x is None) & (y is None):
			x,y = self.wcs.all_world2pix(self.ra,self.dec,0)
			x = int(x + 0.5)
			y = int(y + 0.5)

		ap_tar = np.zeros_like(data[0])
		ap_sky = np.zeros_like(data[0])
		ap_tar[y,x]= 1
		ap_sky[y,x]= 1
		ap_tar = convolve(ap_tar,np.ones((tar_ap,tar_ap))) 
		ap_sky = convolve(ap_sky,np.ones((sky_out,sky_out))) - convolve(ap_sky,np.ones((sky_in,sky_in)))
		ap_sky[ap_sky == 0] = np.nan
		m = sigma_clip((self.ref)*ap_sky,sigma=1).mask
		ap_sky[m] = np.nan
		
		temp = np.nansum(data*ap_tar,axis=(1,2))
		ind = temp < np.percentile(temp,40)
		med = np.nanmedian(data[ind],axis=0)
		med = np.nanmedian(data,axis=0)
		if not diff:
			data = data + self.ref
		if mask is not None:
			ap_sky = mask
			ap_sky[ap_sky==0] = np.nan
		mean_sky, sky_med, sky_std = sigma_clipped_stats(ap_sky*data,axis=(1,2))
		#sky_med = np.nanmedian(ap_sky*data,axis=(1,2))
		#sky_std = np.nanstd(ap_sky*data,axis=(1,2))
		if phot_method == 'aperture':
			if self.diff:
				tar = np.nansum(data*ap_tar,axis=(1,2))
			else:
				tar = np.nansum((data+self.ref)*ap_tar,axis=(1,2))
			tar -= sky_med * tar_ap**2
			tar_err = sky_std * tar_ap**2
		if phot_method == 'psf':
			if psf_snap is None:
				psf_snap = 'brightest'

			tar, tar_err = self.psf_photometry(x,y,diff=diff,snap=psf_snap)
		nan_ind = np.where(np.nansum(self.flux,axis=(1,2))==0,True,False)
		nan_ind[self.ref_ind] = False
		tar[nan_ind] = np.nan
		tar_err[nan_ind] = np.nan

		#tar[tar_err > 100] = np.nan
		#sky_med[tar_err > 100] = np.nan
		if self.tpf is not None:
			time = self.tpf.time.mjd
		lc = np.array([time, tar, tar_err])
		sky = np.array([time, sky_med, sky_std])
		
		if plot:
			self.dif_diag_plot(ap_tar,ap_sky,lc = lc,sky=sky,data=data)
			plt.show()
			if savename is not None:
				plt.savefig(savename + '_diff_diag.pdf', bbox_inches = "tight")
		return lc, sky

	def dif_diag_plot(self,ap_tar,ap_sky,lc=None,sky=None,data=None):
		"""
		Makes a plot showing the target light curve, sky, and difference image at the brightest point
		in the target lc.

		Parameters
		----------
		ap_tar : array
			Aperture to perform photometry on.
		ap_sky : array
			Aperture to perform sky photometry on.
		lc : array, optional
			Light curve of the target to plot. If None, then the assigned lc is used. The default is None.
		sky : array, optional
			Light curve of the sky. The default is None.
		data : array, optional
			Array of images to use in creating the light curve. The default is None.

		Returns
		-------
		Figure.

		"""

		if lc is None:
			lc = self.lc
		if sky is None:
			sky = self.sky
		if data is None:
			data = self.flux
		plt.figure(figsize=(3*fig_width,1*fig_width))
		plt.subplot(121)
		plt.fill_between(lc[0],sky[1]-sky[2],sky[1]+sky[2],alpha=.5,color='C1')
		plt.plot(sky[0],sky[1],'C3.',label='Sky')
		plt.fill_between(lc[0],lc[1]-lc[2],lc[1]+lc[2],alpha=.5,color='C0')
		plt.plot(lc[0],lc[1],'C0.',label='Target')
		binned = self.bin_data(lc=lc)
		plt.plot(binned[0],binned[1],'C2.',label='6hr bin')
		plt.xlabel('Time (MJD)',fontsize=15)
		plt.ylabel('Flux ($e^-/s$)',fontsize=15)
		plt.legend(loc=4)

		plt.subplot(122)
		ap = ap_tar
		ap[ap==0] = np.nan
		maxind = np.where((np.nanmax(lc[1]) == lc[1]))[0]
		try:
			maxind = maxind[0]
		except:
			pass
		d = data[maxind]
		nonan1 = np.isfinite(d)
		nonan2 = np.isfinite(d*ap)
		plt.imshow(data[maxind],origin='lower',
				   vmin=np.nanpercentile(d,16),
				   vmax=np.nanpercentile(d[nonan2],80),
				   aspect='auto')
		cbar = plt.colorbar()
		cbar.set_label('$e^-/s$',fontsize=15)
		plt.xlabel('Column',fontsize=15)
		plt.ylabel('Row',fontsize=15)
		
		#plt.imshow(ap,origin='lower',alpha = 0.2)
		#plt.imshow(ap_sky,origin='lower',alpha = 0.8,cmap='hot')
		y,x = np.where(ap_sky > 0)
		plt.plot(x,y,'r.',alpha = 0.3)
		
		y,x = np.where(ap > 0)
		plt.plot(x,y,'C1.',alpha = 0.3)

		return

	def plotter(self,lc=None,ax = None,ground=False,time_bin=6/24,xlims=None):
		"""
		Simple plotter for light curves. 

		Parameters
		----------
		lc : array, optional
			light curve to plot. The default is None.
		ax : matplotlib axes, optional
			axes to plot onto. The default is None.
		ground : Bool, optional
			If True, then ZTF data will be plotted alongside TESS. The default is False.
		time_bin : float, optional
			Length of time in days to bin data . The default is 6/24.
		xlims : list, optional
			List of x limits to add to the plot. The default is None.

		Returns
		-------
		Figure.

		"""

		if ground:
			if self.ground.ztf is None:
				self.ground.get_ztf_data()
			if self.lc_units.lower() == 'counts':
				self.to_flux()

		if lc is None:
			lc = self.lc
		av = self.bin_data(lc=lc,time_bin=time_bin)
		if time_bin * 24 == int(time_bin * 24):
			lab = int(time_bin * 24) 
			
		else:
			lab = time_bin *24

		if ax is None:
			plt.figure(figsize=(1.5*fig_width,1*fig_width))
			ax = plt.gca()
		if lc.shape[0] > lc.shape[1]:
			ax.plot(lc[:,0],lc[:,1],'k.',alpha = 0.4,ms=1,label='$TESS$')
			
			ax.plot(av[:,0],av[:,1],'k.',label='$TESS$ {}hr'.format(lab))
		else:
			ax.plot(lc[0],lc[1],'.k',alpha = 0.4,ms=1,label='$TESS$')
			ax.plot(av[0],av[1],'.k',label='$TESS$ {}hr'.format(lab))
		
		if self.lc_units == 'AB mag':
			ax.invert_yaxis()
			if ground & (self.ground.ztf is not None):
				gind = self.ground.ztf.fid.values == 'g'
				rind = self.ground.ztf.fid.values == 'r'
				ztfg = self.ground.ztf.iloc[gind]
				ztfr = self.ground.ztf.iloc[rind]
				ax.scatter(ztfg.mjd,ztfg.maglim,c='C2',s=.5,alpha = 0.6,marker='v',label='ZTF g non-detec')
				ax.scatter(ztfr.mjd,ztfr.maglim,c='r',s=.5,alpha = 0.6,marker='v',label='ZTF r non-detec')

				ax.errorbar(ztfg.mjd, ztfg.mag,yerr = ztfg.mag_e, c='C2', fmt='o', ms= 5, label='ZTF g')
				ax.errorbar(ztfr.mjd, ztfr.mag,yerr = ztfr.mag_e, c='r', fmt='o', ms=5, label='ZTF r')
				ax.set_ylabel('Apparent magnitude',fontsize=15)
		else:
			ax.set_ylabel('Flux (' + self.lc_units + ')',fontsize=15)
			if ground & (self.ground.ztf is not None):
				self.ground.to_flux(flux_type=self.lc_units)
				gind = self.ground.ztf.fid.values == 'g'
				rind = self.ground.ztf.fid.values == 'r'
				ztfg = self.ground.ztf.iloc[gind]
				ztfr = self.ground.ztf.iloc[rind]
				ax.scatter(ztfg.mjd,ztfg.fluxlim,c='C2',alpha = 0.6,s=20,marker='v',label='ZTF g non-detec')
				ax.scatter(ztfr.mjd,ztfr.fluxlim,c='r',alpha = 0.6,s=20,marker='v',label='ZTF r non-detec')

				ax.errorbar(ztfg.mjd, ztfg.flux,yerr = ztfg.flux_e,ms=4, c='C2', fmt='o', label='ZTF g')
				ax.errorbar(ztfr.mjd, ztfr.flux,yerr = ztfr.flux_e, ms=4, c='r', fmt='o', label='ZTF r')

		if xlims is not None:
			try:
				xmin, xmax = xlims
			except:
				m = 'xlim must have have shape 2 which are MJD times'
				raise ValueError(m)
			plt.xlim(xmin,xmax)

			ind = (lc[0] < xmax) & (lc[0] > xmin)

			ymin = np.nanmin(lc[1,ind])
			ymax = np.nanmax(lc[1,ind])

			plt.ylim(1.2*ymin,1.2*ymax)


		ax.set_xlabel('Time (MJD)',fontsize=15 )
		ax.legend()
		return

	def save_lc(self,filename,time_bin=None):
		"""
		Saves the current lightcurve out to csv format, doesn't include flux units.

		Parameters
		----------
		filename : str
			Name for the saved file.
		time_bin : float, optional
			Timeframe in days to bin the TESS data. The default is None.

		Returns
		-------
		csv file.

		"""

		if time_bin is not None:
			l = self.bin_data(time_bin=time_bin)
		else:
			l = self.lc

		lc = self.to_lightkurve(lc = l)
		format='csv'
		filename = filename.split('.csv')[0]
		if format == 'csv':
			lc.to_csv(filename)
		
	def to_lightkurve(self,lc=None,flux_unit=None):
		"""
		Convert TESSreduce light curve into lighkurve.lightcurve object. Flux units are recorded

		Parameters
		----------
		lc : array, optional
			Light curve to convert. The default is None.
		flux_unit : str, optional
			Flux units of the light curve. The default is None.
			Valid options:
				counts
				mjy
				cgs

		Returns
		-------
		light : lightkurve object
			The input lightcurve wrapped in the lightkurve format.

		"""
		if lc is None:
			lc = self.lc
		if flux_unit is None:
			flux_unit = self.lc_units
		if flux_unit.lower() == 'counts':
			unit = u.electron/ u.s
		elif flux_unit.lower() == 'mjy':
			unit = 1e-3 * u.Jy
		elif flux_unit.lower() == 'jy':
			unit = u.Jy
		elif flux_unit.lower() == 'cgs':
			unit = u.erg/u.s/u.cm**2/u.Hz
		else:
			unit = 1
		if lc.shape[0] == 3:
			light = lk.LightCurve(time=Time(lc[0], format='mjd'),flux=lc[1] * unit,flux_err=lc[2] * unit)
		else:
			light = lk.LightCurve(time=Time(lc[0], format='mjd'),flux=lc[1] * unit)
		return light

	def _update_reduction_params(self,align,parallel,calibrate,plot,diff_lc,diff,verbose,
								 corr_correction,imaging):
		"""
		Updates relevant parameters for if reduction functions are called out of order.

		Parameters
		----------
		align : Bool
			Trigger alignment procedure.
		parallel : Bool
			Run in parallel.
		calibrate : Bool
			Calibrate the data.
		plot : Bool
			plot the lightcurve.
		diff_lc : Bool
			Create the differenced light curve .
		diff : Bool
			Run difference imaging.
		verbose : int
			Set verbosity.
		corr_correction : Bool
			Run the background correlation correction step .

		Assigns
		-------
		input options

		"""
		if align is not None:
			self.align = align
		if parallel is not None:
			self.parallel = parallel
		if verbose is not None:
			self.verbose = verbose
		if calibrate is not None:
			self.calibrate = calibrate
		if diff is not None:
			self.diff = diff
		if corr_correction is not None:
			self.corr_correction = corr_correction
		if imaging is not None:
			self.imaging = imaging


	def correlation_corrector(self,limit=0.8):
		"""
		A final corrector that removes the final ~0.5% of the background from pixels that have been 
		interpolated over. Assuning the previously calculated background is a reasonable estimate 
		of what the background is like this function finds the coefficient that when multiplied to the 
		background and subtracted from the flux minimises the correlation between the background and 
		the pixel light curve. This function saves the correlation correction as corr_coeff, and 
		applies the correction to the flux, for all pixels that aren't included as sky pixels. 
		This process seems to do a good job at removing some of the residual background structure 
		that is present in some pixels. 

		Parameters
		----------
		limit : float, optional
			Corrects for correlation coefficents larger than limit. 
			If the flux and the background of tess are correlated (absolute value of correlation coefficent, |r|) 
			to a level higher than limit, a fit to minimize this coefficent is preformed, 
			and the new background and flux values are returned. Default is 0.8.

		Returns
		-------
		None.

		"""
		flux, bkg = multi_correlation_cor(self,limit=limit,cores=self.num_cores)
		self.flux = flux 
		self.bkg = bkg

	def _psf_initialise(self,cutoutSize,loc,time_ind=None,ref=False):
		"""
		For gathering the cutouts and PRF base.

		Parameters
		----------
		cutoutSize : int
			Size of the cutouts in pixels.
		loc : array_like
			Pixel coordinates to evaluate.
		ref : bool, optional
			Toggles whether the reference image is used for calculations. The default is False.

		Returns
		-------
		prf : TESS_PRF Class object
			The effective point-spread function generated from TESS_PRF.
		cutout : TYPE
			DESCRIPTION.

		"""
		if time_ind is None:
			time_ind = np.arange(0,len(self.flux))

		if isinstance(loc[0], (float, np.floating, np.float32, np.float64)):
			loc[0] = int(loc[0] + 0.5)
		if isinstance(loc[1], (float, np.floating, np.float32, np.float64)):
			loc[1] = int(loc[1] + 0.5)
		
		col = self.tpf.column - int(self.size/2-1) + loc[0] # find column and row, when specifying location on a *say* 90x90 px cutout
		row = self.tpf.row - int(self.size/2-1) + loc[1] 
			
		prf = TESS_PRF(self.tpf.camera,self.tpf.ccd,self.tpf.sector,col,row) # initialise psf kernel
		if ref:
			cutout = (self.flux+self.ref)[time_ind,loc[1]-cutoutSize//2:loc[1]+1+cutoutSize//2,loc[0]-cutoutSize//2:loc[0]+1+cutoutSize//2] # gather cutouts
		else:
			cutout = self.flux[time_ind,loc[1]-cutoutSize//2:loc[1]+1+cutoutSize//2,loc[0]-cutoutSize//2:loc[0]+1+cutoutSize//2] # gather cutouts
		return prf, cutout

	def moving_psf_photometry(self,xpos,ypos,size=5,time_ind=None,xlim=2,ylim=2):
		"""
		PSF photometry for moving targets.

		Parameters
		----------
		xpos : array_like
			x pixel locations for the initial guess of the target region.
		ypos : array_like
			y pixel locations for the initial guess of the target region.
		size : int, optional
			Size of pixel cutout to use (should be odd). The default is 5.
		time_ind : array_like, optional
			Indices of the time series to use. The default is None.
		xlim : int, optional
			Width of the cutout in pixels. The default is 2.
		ylim : int, optional
			Height of the cutout in pixels. The default is 2.
		
		Returns
		-------
		flux : array_like
			Flux light curve across entire sector.
		"""
		if time_ind is None:
			if len(xpos) != len(self.flux):
				m = 'If "times" is not specified then xpos must have the same length as flux.'
				raise ValueError(m)
			else:
				time_ind = np.arange(0,len(flux))
			if (len(xpos) != len(time_ind)) | (len(ypos) != len(time_ind)):
				m = 'xpos/ypos and time_ind must be the same length'
				raise ValueError(m)
		inds = np.arange(0,len(xpos))
		if self.parallel:
			prfs, cutouts = zip(*Parallel(n_jobs=self.num_cores)(delayed(par_psf_initialise)(self.flux,self.tpf.camera,self.tpf.ccd,
																   							 self.tpf.sector,self.tpf.column,self.tpf.row,
																							 size,[xpos[i],ypos[i]],time_ind) for i in inds))
		else:
			prfs = []
			cutouts = []
			for i in range(len(time_ind)):
				prf, cutout = self._psf_initialise(size,[xpos[i],ypos[i]],time_ind=time_ind[i])
				prfs += [prf]
				cutouts += [cutout]
		cutouts = np.array(cutouts)
		print('made cutouts')
		if self.parallel:
			flux, pos = zip(*Parallel(n_jobs=self.num_cores)(delayed(par_psf_full)(cutouts[i],prfs[i],self.shift[i],xlim,ylim) for i in inds))
		else:
			flux = []
			pos = []
			for i in range(len(xpos)):
				f, p = par_psf_full(cutouts[i],prfs[i],self.shift[i])
				flux += [f]
				pos += [p]
		flux = np.array(flux)
		pos = np.array(pos)
		pos[0,:] += xpos; pos[1,:] += ypos
		return flux, pos

	def psf_photometry(self,xPix,yPix,size=7,snap='brightest',ext_shift=True,plot=False,diff=None):
		"""
		Main PSF Photometry function

		Parameters
		----------
		xPix : float
			x pixel location for the initial guess of the target region.
		yPix : float
			y pixel location for the initial guess of the target region.
		size : int, optional
			Size of pixel cutout to use (should be odd). The default is 5.
		repFact : int, optional
			Super sampling factor for modelling. The default is 10.
		snap : str or int, optional
			Determines how psf position is fit. The default is 'brightest'.
			Valid Options:
				None = each frame's position will be fit and used when fitting for flux
				'brightest' = the position of the brightest cutout frame will be applied to all subsequent frames
				int = providing an integer allows for explicit choice of which frame to use as position reference
				'ref' = use the reference as the position fit point
		ext_shift : array_like, optional
			External shift in the pixel positions. The default is True.
		ext_shift : TYPE, optional
			DESCRIPTION. The default is True.
		plot : bool, optional
			Whether plots will shown. The default is False.
		diff : bool, optional
			If True then difference imaging will occur. The default is None.

		Returns
		-------
		flux : array_like
			Flux light curve across entire sector.

		"""
		
		if diff is None:
			diff = self.diff
		flux = []

		# if isinstance(xPix,(list,np.ndarray)):
		# 	self.moving_psf_phot() 

		if type(snap) == str:
			if snap == 'all': 
				prf, cutouts = self._psf_initialise(size,(xPix,yPix),ref=(not diff))   # gather base PRF and the array of cutouts data
				inds = np.arange(len(cutouts))
				base = create_psf(prf,size)
				flux, eflux, pos = zip(*Parallel(n_jobs=self.num_cores)(delayed(par_psf_full)(cutouts[i],base,self.shift[i]) for i in inds))

				#prf, cutouts = self._psf_initialise(size,(xPix,yPix))   # gather base PRF and the array of cutouts data
				#xShifts = []
				#yShifts = []
				#for cutout in tqdm(cutouts):
				#	PSF = create_psf(prf,size)
				#	PSF.psf_position(cutout)
				#	PSF.psf_flux(cutout)
				#	flux.append(PSF.flux)
				#	yShifts.append(PSF.source_y)
				#	xShifts.append(PSF.source_x)
				#if plot:
				#	fig,ax = plt.subplots(ncols=3,figsize=(12,4))
				#	ax[0].plot(flux)
				#	ax[0].set_ylabel('Flux')
				#	ax[1].plot(xShifts,marker='.',linestyle=' ')
				#	ax[1].set_ylabel('xShift')
				#	ax[2].plot(yShifts,marker='.',linestyle=' ')
				#	ax[2].set_ylabel('yShift')
			else:
				if snap == 'brightest': # each cutout has position snapped to brightest frame fit position
					prf, cutouts = self._psf_initialise(size,(xPix,yPix),ref=(not diff))   # gather base PRF and the array of cutouts data
					bkg = self.bkg[:,int(yPix),int(xPix)]
					#lowbkg = bkg < np.nanpercentile(bkg,16)
					weight = np.nansum(cutouts[:,int(yPix)-1:int(yPix)+2,int(xPix)-1:int(xPix)+2],axis=(1,2)) / bkg
					weight[np.isnan(weight)] = 0
					ind = np.argmax(abs(weight))
					#ind = np.where(cutouts==np.nanmax(cutouts))[0][0]
					ref = cutouts[ind]
					base = create_psf(prf,size)
					base.psf_position(ref,ext_shift=self.shift[ind])
				elif snap == 'ref':
					prf, cutouts = self._psf_initialise(size,(xPix,yPix),ref=True)   # gather base PRF and the array of cutouts data
					ref = cutouts[self.ref_ind]
					base = create_psf(prf,size)
					base.psf_position(ref)
					if diff:
						_, cutouts = self._psf_initialise(size,(xPix,yPix),ref=False)
				if self.parallel:
					inds = np.arange(len(cutouts))
					flux, eflux = zip(*Parallel(n_jobs=self.num_cores)(delayed(par_psf_flux)(cutouts[i],base,self.shift[i]) for i in inds))
				else:
					for i in range(len(cutouts)):
						flux += [par_psf_flux(cutouts[i],base,self.shift[i])]
			if plot:
				plt.figure()
				plt.plot(flux)
				plt.ylabel('Flux')

			
		elif type(snap) == int:	   # each cutout has position snapped to 'snap' frame fit position (snap is integer)
			base = create_psf(prf,size)
			base.psf_position(cutouts[snap])
			for cutout in cutouts:
				PSF = create_psf(prf,size)
				PSF.source_x = base.source_x
				PSF.source_y = base.source_y
				PSF.psf_flux(cutout)
				flux.append(PSF.flux)
			if plot:
				fig,ax = plt.subplots(ncols=1,figsize=(12,4))
				ax.plot(flux)
				ax.set_ylabel('Flux')
		flux = np.array(flux)
		eflux = np.array(eflux)
		return flux, eflux


	def reduce(self, aper = None, align = None, parallel = None, calibrate=None,
				bin_size = 0, plot = None, mask_scale = 1, ref_start=None, ref_stop=None,
				diff_lc = None,diff=None,verbose=None, tar_ap=3,sky_in=7,sky_out=11,
				moving_mask=None,mask=None,double_shift=False,corr_correction=None,test_seed=None,imaging=None):
		"""
		Reduce the images from the target pixel file and make a light curve with aperture photometry.
		This background subtraction method works well on tpfs > 50x50 pixels.

		Parameters
		----------
		aper : None, list or array_like, optional
			Aperature to do photometry on. The default is None.
		align : TYPE, optional
			DESCRIPTION. The default is None.
		parallel : bool, optional
			If True parallel processing will be used for background estimation and centroid shifts. The default is None.
		calibrate : TYPE, optional
			DESCRIPTION. The default is None.
		bin_size : int, optional
			If > 1 then the lightcurve will be binned by that amount. The default is 0.
		plot : TYPE, optional
			DESCRIPTION. The default is None.
		mask_scale : TYPE, optional
			DESCRIPTION. The default is 1.
		ref_start : TYPE, optional
			DESCRIPTION. The default is None.
		ref_stop : TYPE, optional
			DESCRIPTION. The default is None.
		diff_lc : TYPE, optional
			DESCRIPTION. The default is None.
		diff : TYPE, optional
			DESCRIPTION. The default is None.
		verbose : TYPE, optional
			DESCRIPTION. The default is None.
		tar_ap : TYPE, optional
			DESCRIPTION. The default is 3.
		sky_in : TYPE, optional
			DESCRIPTION. The default is 7.
		sky_out : TYPE, optional
			DESCRIPTION. The default is 11.
		moving_mask : TYPE, optional
			DESCRIPTION. The default is None.
		mask : TYPE, optional
			DESCRIPTION. The default is None.
		double_shift : TYPE, optional
			DESCRIPTION. The default is False.
		corr_correction : TYPE, optional
			DESCRIPTION. The default is None.
		test_seed : TYPE, optional
			DESCRIPTION. The default is None.

		Raises
		------
		ValueError
			DESCRIPTION.

		Returns
		-------
		None.

		"""
		# make reference
		try:
			self._update_reduction_params(align, parallel, calibrate, plot, diff_lc, diff, verbose,corr_correction,imaging)

			if (self.flux.shape[1] < 30) & (self.flux.shape[2] < 30):
				small = True	
			else:
				small = False

			if small & self.align:
				print('Unlikely to get good shifts from a small tpf, so shift has been set to False')
				self.align = False

			self.get_ref(ref_start,ref_stop)
			if self.verbose > 0:
				print('made reference')
			# make source mask
			if mask is None:
				self.make_mask(catalogue_path=self._catalogue_path,maglim=18,strapsize=7,scale=mask_scale)
				frac = np.nansum((self.mask == 0) * 1.) / (self.mask.shape[0] * self.mask.shape[1])
				#print('mask frac ',frac)
				if frac < 0.05:
					print('!!!WARNING!!! mask is too dense, lowering mask_scale to 0.5, and raising maglim to 15. Background quality will be reduced.')
					self.make_mask(catalogue_path=self._catalogue_path,maglim=15,strapsize=7,scale=0.5)
				if self.verbose > 0:
					print('made source mask')
			else:
				self.mask = mask
				if self.verbose > 0:
					print('assigned source mask')
			# calculate background for each frame
			if self.verbose > 0:
				print('calculating background')
			
			# calculate the background
			self.background()
			

			if np.isnan(self.bkg).all():
				# check to see if the background worked
				raise ValueError('bkg all nans')
			
			flux = strip_units(self.flux)
			# subtract background from unitless flux
			self.flux = flux - self.bkg
			# get a ref with low background
			self.ref = deepcopy(self.flux[self.ref_ind])
			if self.verbose > 0:
				print('background subtracted')
			
			
			if np.isnan(self.flux).all():
				raise ValueError('flux all nans')

			if self.align:
				if self.verbose > 0:
					print('aligning images')
				
				try:
					#self.centroids_shifts_starfind()
					#if double_shift:
					#self.shift_images()
					#self.ref = deepcopy(self.flux[self.ref_ind])
					self.fit_shift()
					#self.shift_images()
					
				except:
					print('Something went wrong, switching to serial')
					self.parallel = False
					#self.centroids_shifts_starfind()
					self.fit_shift()
					#self.fit_shift()
				#self.fit_shift()
			else:
				self.shift = np.zeros((len(self.flux),2))
			
			if not self.diff:
				if self.align:
					self.shift_images()
					self.flux[np.nansum(self.tpf.flux.value,axis=(1,2))==0] = np.nan
					if self.verbose > 0:
						print('images shifted')

			if self.diff:
				if self.verbose > 0:
					print('!!Re-running for difference image!!')
				# reseting to do diffim 
				self._flux_aligned = deepcopy(self.flux)
				self.flux = strip_units(self.tpf.flux)
				self.flux = self.flux / self.qe

				if self.align:
					self.shift_images()

					if self.verbose > 0:
						print('shifting images')

				if test_seed is not None:
					self.flux += test_seed
				self.flux[np.nansum(self.tpf.flux.value,axis=(1,2))==0] = np.nan
				# subtract reference
				self.ref = deepcopy(self.flux[self.ref_ind])
				self.flux -= self.ref

				self.ref -= self.bkg[self.ref_ind]
				# remake mask
				self.make_mask(catalogue_path=self._catalogue_path,maglim=18,strapsize=7,scale=mask_scale*.8,useref=False)#Source_mask(ref,grid=0)
				frac = np.nansum((self.mask== 0) * 1.) / (self.mask.shape[0] * self.mask.shape[1])
				#print('mask frac ',frac)
				if frac < 0.05:
					print('!!!WARNING!!! mask is too dense, lowering mask_scale to 0.5, and raising maglim to 15. Background quality will be reduced.')
					self.make_mask(catalogue_path=self._catalogue_path,maglim=15,strapsize=7,scale=0.5)
				# assuming that the target is in the centre, so masking it out 
				#m_tar = np.zeros_like(self.mask,dtype=int)
				#m_tar[self.ref.shape[0]//2,self.ref.shape[1]//2]= 1
				#m_tar = convolve(m_tar,np.ones((5,5)))
				#self.mask = self.mask | m_tar
				if moving_mask is not None:
					moving_mask = moving_mask > 0
					temp = np.zeros_like(self.flux,dtype=int)
					temp[:,:,:] = self.mask
					self.mask = temp | moving_mask

				if self.verbose > 0:
					print('remade mask')
				# background
				if self.verbose > 0:
					print('background')
				self.bkg_orig = deepcopy(self.bkg)
				self.background(calc_qe = False,strap_iso = False,source_hunt=self._sourcehunt,gauss_smooth=1,interpolate=False)
				self._grad_bkg_clip()
				self.flux -= self.bkg
				if self.corr_correction:
					if self.verbose > 0:
						print('background correlation correction')
					self.correlation_corrector()


			if self.calibrate:
				print('field calibration')
				self.field_calibrate()

			
			self.lc, self.sky = self.diff_lc(plot=self.plot,diff=self.diff,tar_ap=tar_ap,sky_in=sky_in,sky_out=sky_out)

			if self.imaging:
				# if self.verbose > 0:
				# 	print('Retrieving external photometry')
				self.external_photometry()

		except Exception:
			print(traceback.format_exc())

		
	def external_photometry(self,size=50,phot=None):
		"""
		Perform aperture photometry on an external source

		Parameters
		----------
		size : int, optional
			Size of the cutout to use in arcseconds. The default is 50.
		phot : str, optional
			Type of photometry to choose, skymapper or Panstarrs. The default is None.
		"""

		event_cutout((self.ra,self.dec),size,phot)

	def make_lc(self,aperture = None,bin_size=0,zeropoint=None,scale='counts',clip = False):
		"""
		Perform aperature photometry on a time series of images

		Parameters
		----------
		aperture : array_like, optional
			An array of aperture sizes to use. The default is None.
		bin_size : int, optional
			Number of points to average. The default is 0.
		zeropoint : float, optional
			The calculated zeropoint of the data. The default is None.
		scale : bool, optional
			If True the light curve will be normalised to the median. The default is 'counts'.
			Valid options = [counts, magnitude, flux, normalise]
		clip : bool, optional
			Whether to clip the data. The default is False.

		Returns
		-------
		self.lc : array_like
			light curve for the pixels defined by the aperture

		"""
		
		# hack solution for new lightkurve
		flux = strip_units(self.flux)
		t = self.tpf.time.mjd

		if type(aperture) == type(None):
			aper = np.zeros_like(flux[0])
			aper[int(aper.shape[0]/2),int(aper.shape[1]/2)] = 1
			aper = convolve(aper,np.ones((3,3)))
			temp = np.zeros_like(flux[0])
		elif type(aperture) == list:
			temp = np.zeros_like(flux[0])
			temp[aperture[0],aperture[1]] = 1 
			aper = temp
		elif type(aperture) == np.ndarray:
			aper = aperture * 1.
			 
		lc = Lightcurve(flux,aper)   #,scale = scale)
		if clip:
			mask = ~sigma_mask(lc)
			lc[mask] = np.nan
		if bin_size > 1:
			lc, t = self.bin_data(t,lc,bin_size)
		lc = np.array([t,lc])
		if (zeropoint is not None) & (scale=='mag'):
			lc[1,:] = -2.5*np.log10(lc[1,:]) + zeropoint
		self.lc = lc

	def lc_events(self,lc = None,err=None,duration=10,sig=5):
		"""
		Use clustering to detect individual high SNR events in a light curve.
		Clustering isn't incredibly robust, so it could be better.

		Parameters
		----------
		lc : array_like, optional
			lightcurve with the shape of (2,n), where the first index is time and the second is 
			flux. The default is None.
		err : array_like, optional
			Flux error to be used in weighting of events. The default is None.
		duration : int, optional
			How long an event needs to last for before being detected. The default is 10.
		sig : Float, optional
			Significance of the detection above the background. The default is 5.

		Returns
		-------
		self.events : list
			list of light curves for all identified events 

		"""

		if lc is None:
			lc = deepcopy(self.lc)
		if lc.shape[0] > lc.shape[1]:
			lc = lc.T
		ind = np.isfinite(lc[1])
		lc = lc[:,ind]
		mask = Cluster_cut(lc,err=err,sig=sig)
		outliers = Identify_masks(mask)
		good = np.nansum(outliers,axis=1) > duration
		outliers = outliers[good]
		print('Found {} events longer than {} frames at {} sigma'.format(outliers.shape[0],duration,sig))
		temp = outliers * lc[1][np.newaxis,:]
		lcs = []
		for event in temp:
			l = (self.lc[:2]).copy()
			l[1,:] = np.nan
			l[1,ind] = event
			lcs += [l]
		lcs = np.array(lcs)
		lcs[lcs == 0] = np.nan
		self.events = lcs

	def event_plotter(self,**kwargs):
		"""
		Lazy plotting tool for checking the detected events.

		Parameters
		----------
		**kwargs : Various
			Keyword arguments, takes arguments for lightcurve events.

		Returns
		-------
		None.

		"""
		if self.events is None:
			self.lc_events(**kwargs)
		plt.figure()
		plt.plot(self.lc[0],self.lc[1],'k.')
		for i in range(len(self.events)):
			plt.plot(self.events[i,0],self.events[i,1],'*',label='Event {}'.format(i))
		plt.xlabel('MJD')
		plt.ylabel('Flux')

	def detrend_transient(self,lc=None,err=None,Mask=None,variable=False,sig = 5, 
						  sig_up = 3, sig_low = 10, tail_length='auto',plot=False):
		"""
		Removes all long term stellar variability, while preserving flares. Input a light curve 
		with shape (2,n) and it should work!

		Parameters
		----------
		lc : array_like, optional
			lightcurve with the shape of (2,n), where the first index is time and the second is 
			flux. The default is None.
		err : array_like, optional
			Flux error to be used in weighting of events of size (n,). The default is None.
		Mask : array_like, optional
			1d one dimensional mask of the lightcurve to not be included in the detrending. The default is None.
		variable : bool, optional
			Determine whether the object is variable. The default is False.
		sig : float, optional
			Significance of the event before it gets excluded. The default is None.
		sig_up : Float, optional
			Upper sigma clip value . The default is 5.
		sig_low : Float, optional
			Lower sigma clip value. The default is 10.
		tail_length : str OR int, optional
			Option for setting the buffer zone of points after the peak. If it is 'auto' it 
			will be determined through functions, but if its an int then it will take the given 
			value as the buffer tail length for fine tuning. The default is ''.

		Raises
		------
		ValueError
			DESCRIPTION.

		Returns
		-------
		detrend : array_like
			Lightcurve with the stellar trends subtracted.

		"""
		# Make a smoothing value with a significant portion of the total 
		
		if lc is None:
			lc = self.lc[:2]
		nonan = np.isfinite(lc[1])
		lc = lc[:,nonan]

		if (err is None) & (self.lc.shape[0] > 2):
			err = self.lc[2]
			err = err[nonan]

		trends = np.zeros(lc.shape[1])
		break_inds = Multiple_day_breaks(lc)
		#lc[Mask] = np.nan

		if variable:
			size = int(lc.shape[1] * 0.1)
			if size % 2 == 0: size += 1

			finite = np.isfinite(lc[1])
			smooth = savgol_filter(lc[1,finite],size,1)		
			# interpolate the smoothed data over the missing time values
			f1 = interp1d(lc[0,finite], smooth, kind='linear',fill_value='extrapolate')
			smooth = f1(lc[0])
			lc2 = lc.copy()
			lc2[1] = lc2[1] - smooth
			try:
				mask = Cluster_cut(lc2,err=err,sig=sig)
			except:
				print('could not cluster')
				mask = sig_err(lc2[1],err,sig=sig)
			#sigma_clip(lc[1]-smooth,sigma=sig,sigma_upper=sig_up,
			#					sigma_lower=sig_low,masked=True).mask
		else:
			try:
				mask = Cluster_cut(lc,err=err,sig=sig)
			except:
				print('could not cluster')
				mask = sig_err(lc[1],err,sig=sig)

		ind = np.where(mask)[0]
		masked = lc.copy()
		# Mask out all peaks, with a lead in of 5 frames and tail of 100 to account for decay
		# todo: use findpeaks to get height estimates and change the buffers accordingly
		if type(tail_length) == str:
			if tail_length == 'auto':
				#m = auto_tail(lc,mask,err)
				masked[:,mask] = np.nan


			else:
				if lc.shape[1] > 4000:
					tail_length = 100
					start_length = 1
				else:
					tail_length = 10
				for i in ind:
					masked[:,i-5:i+tail_length] = np.nan
		else:
			tail_length = int(tail_length)
			if type(tail_length) != int:
				raise ValueError("tail_length must be either 'auto' or an integer")
			for i in ind:
				masked[:,i-5:i+tail_length] = np.nan


		## Hack solution doesnt need to worry about interpolation. Assumes that stellar variability 
		## is largely continuous over the missing data regions.
		#f1 = interp1d(lc[0,finite], lc[1,finite], kind='linear',fill_value='extrapolate')
		#interp = f1(lc[0,:])

		# Smooth the remaining data, assuming its effectively a continuous data set (no gaps)
		size = int(lc.shape[1] * 0.01)
		if size % 2 == 0: 
			size += 1
		for i in range(len(break_inds)-1):
			section = lc[:,break_inds[i]:break_inds[i+1]]

			mask_section = masked[:,break_inds[i]:break_inds[i+1]]
			if np.nansum(mask_section) < 10:
				mask_section[1,:] = np.nanmedian(masked[1,:])
				if np.nansum(abs(mask_section)) < 10:
					mask_section[1,:] = np.nanmedian(section)
			
			if np.isnan(mask_section[1,0]):
				mask_section[1,0] = np.nanmedian(mask_section[1])
			if np.isnan(mask_section[1,-1]):
				mask_section[1,-1] = np.nanmedian(mask_section[1])
			finite = np.isfinite(mask_section[1])
			smooth = savgol_filter(mask_section[1,finite],size,1)

			# interpolate the smoothed data over the missing time values
			f1 = interp1d(section[0,finite], smooth, kind='linear',fill_value='extrapolate')
			trends[break_inds[i]:break_inds[i+1]] = f1(section[0])
			
		if plot:
			plt.figure()
			plt.plot(self.lc[0],self.lc[1])
			plt.plot(self.lc[0,nonan],trends,'.')
		detrend = deepcopy(self.lc)
		detrend[1,nonan] -= trends
		return detrend

	def detrend_stellar_var(self,lc=None,err=None,Mask=None,variable=False,sig = None, 
							sig_up = 5, sig_low = 10, tail_length=''):
		"""
		Removes all long term stellar variability, while preserving flares. Input a light curve 
		with shape (2,n) and it should work!

		Parameters
		----------
		lc : array_like, optional
			lightcurve with the shape of (2,n), where the first index is time and the second is 
			flux. The default is None.
		err : array_like, optional
			Flux error to be used in weighting of events of size (n,). The default is None.
		Mask : array_like, optional
			1d one dimensional mask of the lightcurve to not be included in the detrending. The default is None.
		variable : bool, optional
			Determine whether the object is variable. The default is False.
		sig : float, optional
			Significance of the event before it gets excluded. The default is None.
		sig_up : Float, optional
			Upper sigma clip value . The default is 5.
		sig_low : Float, optional
			Lower sigma clip value. The default is 10.
		tail_length : str OR int, optional
			Option for setting the buffer zone of points after the peak. If it is 'auto' it 
			will be determined through functions, but if its an int then it will take the given 
			value as the buffer tail length for fine tuning. The default is ''.

		Raises
		------
		ValueError
			"tail_length must be either 'auto' or an integer".

		Returns
		-------
		detrend : array_like
			Lightcurve with the stellar trends subtracted.

		"""

		# Make a smoothing value with a significant portion of the total 
		if lc is None:
			lc = self.lc[:2]
		nonan = np.isfinite(lc[1])
		lc = lc[:,nonan]

		if (err is None) & (self.lc.shape[0] > 2):
			err = self.lc[2]
			err = err[nonan]

		trends = np.zeros(lc.shape[1])
		break_inds = Multiple_day_breaks(lc)
		self._midsector_break = break_inds
		#lc[Mask] = np.nan
		
		if variable:
			size = int(lc.shape[1] * 0.08)
			if size % 2 == 0: size += 1

			finite = np.isfinite(lc[1])
			smooth = savgol_filter(lc[1,finite],size,1)		
			# interpolate the smoothed data over the missing time values
			f1 = interp1d(lc[0,finite], smooth, kind='linear',fill_value='extrapolate')
			smooth = f1(lc[0])
			mask = sig_err(lc[1]-smooth,err,sig=sig)
			#sigma_clip(lc[1]-smooth,sigma=sig,sigma_upper=sig_up,
			#					sigma_lower=sig_low,masked=True).mask
		else:
			mask = sig_err(lc[1],err,sig=sig)
			
		ind = np.where(mask)[0]
		masked = lc.copy()
		# Mask out all peaks, with a lead in of 5 frames and tail of 100 to account for decay
		# todo: use findpeaks to get height estimates and change the buffers accordingly
		if type(tail_length) == str:
			if tail_length == 'auto':
				
				m = auto_tail(lc,mask,err)
				masked[:,~m] = np.nan
				
				
			else:
				if lc.shape[1] > 4000:
					tail_length = 100
					start_length = 1
				else:
					tail_length = 10
				for i in ind:
					masked[:,i-5:i+tail_length] = np.nan
		else:
			tail_length = int(tail_length)
			if type(tail_length) != int:
				raise ValueError("tail_length must be either 'auto' or an integer")
			for i in ind:
				masked[:,i-5:i+tail_length] = np.nan
		
		
		## Hack solution doesnt need to worry about interpolation. Assumes that stellar variability 
		## is largely continuous over the missing data regions.
		#f1 = interp1d(lc[0,finite], lc[1,finite], kind='linear',fill_value='extrapolate')
		#interp = f1(lc[0,:])

		# Smooth the remaining data, assuming its effectively a continuous data set (no gaps)
		size = int(lc.shape[1] * 0.005)
		if size % 2 == 0: 
			size += 1
		for i in range(len(break_inds)-1):
			section = lc[:,break_inds[i]:break_inds[i+1]]
			finite = np.isfinite(masked[1,break_inds[i]:break_inds[i+1]])
			smooth = savgol_filter(section[1,finite],size,1)
			
			# interpolate the smoothed data over the missing time values
			f1 = interp1d(section[0,finite], smooth, kind='linear',fill_value='extrapolate')
			trends[break_inds[i]:break_inds[i+1]] = f1(section[0])
		# We now have a trend that should remove stellar variability, excluding flares.
		detrend = deepcopy(lc)
		detrend[1,:] = lc[1,:] - trends
		return detrend

	def bin_interp(self,lc=None,time_bin=6/24):
		"""
		Grabs the binned data and interpolates it to the original time values.

		Parameters
		----------
		lc : array_like, optional
			The lightcurve of the target in the form of three rows: mjd, flux, flux error. The default is None.	
		time_bin : float, optional
			The time (in days) for each bin to be for averaging data. The default is 6/24.

		Returns
		-------
		smooth : array_like
			Binned data in the form of three rows: mjd, flux, flux error.

		"""
		if lc is None:
			lc = self.lc
		if lc.shape[0] > lc.shape[1]:
			lc = lc.T
		binned = self.bin_data(lc=lc,time_bin=time_bin)
		finite = np.isfinite(binned[1])
		f1 = interp1d(binned[0,finite], binned[1,finite], kind='linear',fill_value='extrapolate')
		smooth = f1(lc[0])
		return smooth

	def detrend_star(self,lc=None):
		"""
		Removes trends, e.g. background or stellar variability from the lightcurve data.

		Parameters
		----------
		lc : array_like, optional
			The lightcurve of the target in the form of three rows: mjd, flux, flux error. The default is None.

		Returns
		-------
		detrended : array_like
			The lightcurve data where datapoints with strict gradient changes are ignored and a 
			savitsky-savgol filter is then applied to smooth out the data.

		"""
		if lc is None:
			lc = self.lc
		if lc.shape[0] > lc.shape[1]:
			lc = lc.T
		# clip outliers with grads 
		raw_flux = lc[1]
		rad = grads_rad(raw_flux)
		ind = (rad > np.nanmedian(rad)+5*np.nanstd(rad))
		flux = deepcopy(raw_flux)
		flux[ind] = np.nan
		smooth = self.bin_interp(lc = np.array([lc[0],flux]))
		
		sub = flux - smooth
		
		rad = grad_flux_rad(sub)
		ind = rad > np.nanmedian(rad)+2*np.nanstd(rad)
		
		mask = ind * 1
		mask = convolve(mask,np.ones((3))) > 0
		
		temp = deepcopy(lc)
		temp[1,mask] = np.nan

		size = int(lc.shape[1] * 0.05)
		if size % 2 == 0: size += 1
		finite = np.isfinite(temp[1])
		smooth = savgol_filter(temp[1,finite],size,2)
		f1 = interp1d(temp[0,finite], smooth, kind='linear',fill_value='extrapolate')
		smooth = f1(temp[0])
		
		detrended = deepcopy(lc)
		detrended[1] -= smooth
		return detrended


	### serious calibration 
	def isolated_star_lcs(self):
		"""
		Serious calibration for isolated stars grabbing from either the skymapper or panstarrs database.
		This gives the lightcurves for the TESS target and the isolated stars in the field.

		Returns
		-------
		final_flux : array
			The final flux/TESS lightcurves for the stars in the final_d subset.
		final_d : array
			A subset of the skymapper or panstarrs observations/data tables that are not strongly 
			red in colour and below certain apparent magnitudes.

		"""
		if self.dec < -30:
			if self.verbose > 0:
				print('target is below -30 dec, calibrating to SkyMapper photometry.')
			table = Get_Catalogue(self.tpf,Catalog='skymapper')
			table = Skymapper_df(table)
			system = 'skymapper'
		else:
			if self.verbose > 0:
				print('target is above -30 dec, calibrating to PS1 photometry.')
			table = Get_Catalogue(self.tpf,Catalog='ps1')
			system = 'ps1'

		if self.diff:
			tflux = self.flux + self.ref
		else:
			tflux = self.flux
			

		ind = (table.imag.values < 19) & (table.imag.values > 14)
		tab = table.iloc[ind]
		x,y = self.wcs.all_world2pix(tab.RAJ2000.values,tab.DEJ2000.values,0)
		tab['col'] = x
		tab['row'] = y
		
		e, dat = Tonry_reduce(tab,system=system)
		self.ebv = e[0]

		gr = (dat.gmag - dat.rmag).values
		ind = (gr < 1) & (dat.imag.values < 17)
		d = dat.iloc[ind]
		x,y = self.wcs.all_world2pix(d.RAJ2000.values,d.DEJ2000.values,0)
		d['col'] = x
		d['row'] = y
		pos_ind = (1 < x) & (x < self.ref.shape[0]-2) & (1 < y) & (y < self.ref.shape[0]-2)
		d = d.iloc[pos_ind]

		# account for crowding 
		for i in range(len(d)):
			x = d.col.values[i]
			y = d.row.values[i]
			
			dist = np.sqrt((tab.col.values-x)**2 + (tab.row.values-y)**2)
			
			ind = dist < 1.5
			close = tab.iloc[ind]
			
			d['gmag'].iloc[i] = -2.5*np.log10(np.nansum(mag2flux(close.gmag.values,25))) + 25
			d['rmag'].iloc[i] = -2.5*np.log10(np.nansum(mag2flux(close.rmag.values,25))) + 25
			d['imag'].iloc[i] = -2.5*np.log10(np.nansum(mag2flux(close.imag.values,25))) + 25
			d['zmag'].iloc[i] = -2.5*np.log10(np.nansum(mag2flux(close.zmag.values,25))) + 25
			if system == 'ps1':
				d['ymag'].iloc[i] = -2.5*np.log10(np.nansum(mag2flux(close.ymag.values,25))) + 25
		# convert to tess mags
		if len(d) < 10:
			print('!!!WARNING!!! field calibration is unreliable, using the default zp = 20.44')
			self.zp = 20.44
			self.zp_e = 0.5
			# backup for when messing around with flux later
			self.tzp = 20.44
			self.tzp_e = 0.5
			return
		if system == 'ps1':
			d = PS1_to_TESS_mag(d,ebv=self.ebv)
		else:
			d = SM_to_TESS_mag(d,ebv=self.ebv)

		
		flux = []
		eflux = []
		eind = np.zeros(len(d))
		for i in range(len(d)):
			#if self.phot_method == 'aperture':
			mask = np.zeros_like(self.ref)
			mask[int(d.row.values[i] + .5),int(d.col.values[i] + .5)] = 1
			mask = convolve(mask,np.ones((3,3)))
			flux += [np.nansum(tflux*mask,axis=(1,2))]
			m2 = np.zeros_like(self.ref)
			m2[int(d.row.values[i] + .5),int(d.col.values[i] + .5)] = 1
			m2 = convolve(m2,np.ones((7,7))) - convolve(m2,np.ones((5,5)))
			eflux += [np.nansum(tflux*m2,axis=(1,2))]
			mag = -2.5*np.log10(np.nansum((self.ref*m2))) + 20.44
			#elif self.phot_method == 'psf':
			#	self.psf_photometry(xPix=d.col.values[i],yPix=d.row.values[i],snap=None,diff=False)
			
			if (mag <= d.tmag.values[i]+1):# | (mag <= 17):
				eind[i] = 1
		eind = eind == 0
		flux = np.array(flux)
		eflux = np.array(eflux)
		#eind = abs(eflux) > 20
		flux[~eind] = np.nan

		final_d = d.iloc[eind]
		final_flux = flux[eind]

		return final_flux, final_d

	def field_calibrate(self,zp_single=True,plot=None,savename=None):
		"""
		In-situ flux calibration for TESSreduce light curves. This uses the
		flux calibration method developed in Ridden-Harper et al. 2021 where a broadband 
		filter is reconstructed by a linear combination of PS1 filters + a non linear colour term.
		Here, we calibrate to all PS1 stars in the tpf region by first calculating the 
		stellar extinction in E(B-V) using stellar locus regression. We then identify all reasonably 
		isolated stars with g-r < 1 and i < 17 in the TPF. For each isolated source we calculate the
		expected TESS magnitude, including all sources within 2.5 pixels (52.5''), and compare 
		that to TESS aperture photometry. Averaging together all valid sources gives us a 
		good representation of the TESS zeropoint. 

		Parameters
		----------
		zp_single : bool, optional
			valid options are True or False. The default is True.		
				if True all points through time are averaged to a single zp
				if False then the zp is time varying, creating an extra photometric correction
				for light curves, but with increased error in the zp.
		plot : bool, optional
			If True then diagnostic plots will be created. The default is None.
		savename : str, optional
			The name used for the saving files. The default is None.

		Returns
		-------
		None.
			self.ebv : float 
				Estimated E(B-V) extinction from stellar locus regression
			self.zp/tzp : float
				TESS photometric zeropoint
			self.zp_e/tzp_e : float
				Error in the photometric zeropoint

		"""

		if plot is None:
			plot = self.diagnostic_plot
		if savename is None:
			savename = self.savename
		if self.dec < -30:
			if self.verbose > 0:
				print('target is below -30 dec, calibrating to SkyMapper photometry.')
			table = Get_Catalogue(self.tpf,Catalog='skymapper')
			#table = Skymapper_df(table)
			system = 'skymapper'
		else:
			if self.verbose > 0:
				print('target is above -30 dec, calibrating to PS1 photometry.')
			table = Get_Catalogue(self.tpf,Catalog='ps1')
			system = 'ps1'
		x,y = self.wcs.all_world2pix(table.RAJ2000.values,table.DEJ2000.values,0)
		table['col'] = x
		table['row'] = y
		self.cat = table
		
		ref = deepcopy(self.ref)
		m = ((self.mask & 1 == 0) & (self.mask & 2 == 0) ) * 1.
		m[m==0] = np.nan
		ref_bkg = np.nanmedian(ref * m)
		
		ref -= ref_bkg
		if self.diff:
			tflux = self.flux + ref
		else:
			tflux = self.flux
			

		ind = (table.imag.values < 19) & (table.imag.values > 14)
		tab = table.iloc[ind]
		
		e, dat = Tonry_reduce(tab,plot=plot,savename=savename,system=system)
		self.ebv = e[0]

		gr = (dat.gmag - dat.rmag).values
		ind = (gr < 1) & (dat.imag.values < 15)
		d = dat.iloc[ind]
		
		x,y = self.wcs.all_world2pix(d.RAJ2000.values,d.DEJ2000.values,0)
		d['col'] = x
		d['row'] = y
		pos_ind = (5 < x) & (x < self.ref.shape[1]-5) & (5 < y) & (y < self.ref.shape[0]-5)
		d = d.iloc[pos_ind]
		

		# account for crowding 
		for i in range(len(d)):
			x = d.col.values[i]
			y = d.row.values[i]
			
			dist = np.sqrt((tab.col.values-x)**2 + (tab.row.values-y)**2)
			
			ind = dist < 1.5
			close = tab.iloc[ind]
			
			d['gmag'].iloc[i] = -2.5*np.log10(np.nansum(mag2flux(close.gmag.values,25))) + 25
			d['rmag'].iloc[i] = -2.5*np.log10(np.nansum(mag2flux(close.rmag.values,25))) + 25
			d['imag'].iloc[i] = -2.5*np.log10(np.nansum(mag2flux(close.imag.values,25))) + 25
			d['zmag'].iloc[i] = -2.5*np.log10(np.nansum(mag2flux(close.zmag.values,25))) + 25
			if system == 'ps1':
				d['ymag'].iloc[i] = -2.5*np.log10(np.nansum(mag2flux(close.ymag.values,25))) + 25
		# convert to tess mags
		if len(d) < 10:
			print('!!!WARNING!!! field calibration is unreliable, using the default zp = 20.44')
			self.zp = 20.44
			self.zp_e = 0.05
			# backup for when messing around with flux later
			self.tzp = 20.44
			self.tzp_e = 0.05
			return
		if system == 'ps1':
			d = PS1_to_TESS_mag(d,ebv=self.ebv)
		else:
			d = SM_to_TESS_mag(d,ebv=self.ebv)

		
		flux = []
		eflux = []
		eind = np.zeros(len(d))
		for i in range(len(d)):
			mask = np.zeros_like(self.ref)
			xx = int(d.col.values[i] + .5); yy = int(d.row.values[i] + .5)
			mask[yy,xx] = 1
			mask = convolve(mask,np.ones((3,3)))
			if self.phot_method == 'aperture':
				flux += [np.nansum(tflux*mask,axis=(1,2))]
			elif self.phot_method == 'psf':
				flux += [self.psf_photometry(xPix=xx,yPix=yy,snap='ref',diff=False)]
			m2 = np.zeros_like(self.ref)
			m2[int(d.row.values[i] + .5),int(d.col.values[i] + .5)] = 1
			m2 = convolve(m2,np.ones((7,7))) - convolve(m2,np.ones((5,5)))
			eflux += [np.nansum(tflux*m2,axis=(1,2))]
			mag = -2.5*np.log10(np.nansum((ref*m2))) + 20.44

			
			if (mag <= d.tmag.values[i]+1):# | (mag <= 17):
				eind[i] = 1
		eind = eind == 0

		flux = np.array(flux)
		eflux = np.array(eflux)
		#eind = abs(eflux) > 20
		if self.phot_method == 'aperture':
			flux[~eind] = np.nan
		
		print(type(flux))
		print(flux.shape)
		print(type(d.tmag.values))
		print(d.tmag.values.shape)
		#calculate the zeropoint
		zp = d.tmag.values[:,np.newaxis] + 2.5*np.log10(flux) 
  		# zp = d.tmag.values[:, np.newaxis, np.newaxis]  + 2.5*np.log10(flux) 
		if len(zp) == 0:
			zp = np.array([20.44])
		
		mzp = np.zeros_like(zp[0]) * np.nan
		stdzp = np.zeros_like(zp[0]) * np.nan
		for i in range(zp.shape[1]):
			#averager = calcaverageclass()
			mean, med, std = sigma_clipped_stats(zp[eind,i], sigma=3.0)
			#averager.calcaverage_sigmacutloop(zp[eind,i])
			mzp[i] = med#averager.mean
			stdzp[i] = std#averager.stdev

		#averager = calcaverageclass()
		mean, med, std = sigma_clipped_stats(mzp[np.isfinite(mzp)], sigma=3.0)
		#averager.calcaverage_sigmacutloop(mzp[np.isfinite(mzp)],noise=stdzp[np.isfinite(mzp)])

		if plot:
			plt.figure()
			nonan = np.isfinite(self.ref)
			plt.imshow(ref,origin='lower',vmax = np.percentile(ref[nonan],80),vmin=np.percentile(ref[nonan],10))
			plt.scatter(d.col.iloc[eind],d.row.iloc[eind],color='r')
			plt.title('Calibration sources')
			plt.ylabel('Row',fontsize=15)
			plt.xlabel('Column',fontsize=15)
			plt.colorbar()
			plt.show()
			if savename is not None:
				plt.savefig(savename + 'cal_sources.pdf', bbox_inches = "tight")


			mask = sigma_mask(mzp,3)
			plt.figure(figsize=(3*fig_width,1*fig_width))
			plt.subplot(121)
			plt.hist(mzp[mask],alpha=0.5)
			#plt.axvline(averager.mean,color='C1')
			#plt.axvspan(averager.mean-averager.stdev,averager.mean+averager.stdev,alpha=0.3,color='C1')
			#plt.axvspan(med-std,med+std,alpha=0.3,color='C1')
			med = med
			low = med-std
			high = med+std
			plt.axvline(med,ls='--',color='k')
			plt.axvline(low,ls=':',color='k')
			plt.axvline(high,ls=':',color='k')

			s = '$'+str((np.round(med,3)))+'^{+' + str((np.round(high-med,3)))+'}_{'+str((np.round(low-med,3)))+'}$'
			plt.annotate(s,(.70,.8),fontsize=13,xycoords='axes fraction')
			plt.xlabel('Zeropoint',fontsize=15)
			plt.ylabel('Occurrence',fontsize=15)
			plt.gca().xaxis.set_major_locator(plt.MaxNLocator(6))

			plt.subplot(122)
			plt.plot(self.tpf.time.mjd[mask],mzp[mask],'.',alpha=0.5)
			#plt.axhspan(averager.mean-averager.stdev,averager.mean+averager.stdev,alpha=0.3,color='C1')
			#plt.axhline(averager.mean,color='C1')
			#plt.axhspan(med-std,med+std,alpha=0.3,color='C1')

			plt.axhline(low,color='k',ls=':')
			plt.axhline(high,color='k',ls=':')
			plt.axhline(med,color='k',ls='--')

			plt.ylabel('Zeropoint',fontsize=15)
			plt.xlabel('MJD',fontsize=15)
			plt.tight_layout()
			plt.show()
			if savename is not None:
				plt.savefig(savename + 'cal_zp.pdf', bbox_inches = "tight")

		if zp_single:
			mzp = med#averager.mean
			stdzp = std#averager.stdev
			compare = abs(mzp-20.44) > 2

		else:
			zp = np.nanmedian(zp,axis=0)
			mzp,stdzp = smooth_zp(zp, self.tpf.time.mjd)
			compare = (abs(mzp-20.44) > 2).any()

		if compare:
			print('!!!WARNING!!! field calibration is unreliable, using the default zp = 20.44')
			self.zp = 20.44
			self.zp_e = 0.5
			# backup for when messing around with flux later
			self.tzp = 20.44
			self.tzp_e = 0.5
		else:
			self.zp = mzp
			self.zp_e = stdzp
			# backup for when messing around with flux later
			self.tzp = mzp
			self.tzp_e = stdzp

		return

	def to_mag(self,zp=None,zp_e=0):
		"""
		Convert the TESS lc into magnitude space.
		This is non reversible, since negative values will be lost.

		Parameters
		----------
		zp : float, optional
			Zeropoint to use for conversion. If None, use the default zp from the object. The default is None.
		zp_e : float, optional
			Error on the zeropoint to use for conversion. If None, use the default zp_e from the object. The default is 0.

		Returns
		-------
		lc : array_like
			Lightcurve in magnitude space. mjd, magnitude, magnitude_error.

		"""

		if (zp is None) & (self.zp is not None):
			zp = self.zp
			zp_e = self.zp_e
		elif (zp is None) & (self.zp is None):
			self.field_calibrate()
			zp = self.zp
			zp_e = self.zp_e

		mag = -2.5*np.log10(self.lc[1]) + zp
		mag_e = np.sqrt((2.5/np.log(10) * self.lc[2]/self.lc[1])**2 + zp_e**2)

		lc = deepcopy(self.lc)
		lc[1] = mag
		lc[2] = mag_e

		#self.lc[1] = mag
		#self.lc[2] = mag_e
		#self.lc_units = 'AB mag'
		return lc

	def to_flux(self,zp=None,zp_e=0,flux_type='mjy',plot=False):
		"""
		Convert the TESS lc to physical flux. Either the field calibrated zp 
		or a given zp can be used. 

		Parameters
		----------
		zp : float, optional
			TESS zeropoint. The default is None.
		zp_e : float, optional
			Error in the TESS zeropoint. The default is 0.
		flux_type : str, optional
			The units for the flux output. The default is 'mjy'.
			Valid options:
				mjy 
				jy
				erg/cgs
				tess/counts
		plot : bool, optional
			Plot the field calibration figures, if used. The default is False.

		Raises
		------
		ValueError
			Flux Type is not a valid option, please choose from:\njy\nmjy\ncgs/erg\ntess/counts'.

		Returns
		-------
		None.
			self.lc : array_like
				Returns the lightcurve in flux units, with mjd, flux, flux error being the three rows.
			self.zp : float
				Returns the zeropoint used for the magnitude conversion.
			self.zp_e : float 
				Returns the error on the zeropoint used for the magnitude conversion.
			self.lc_units : str
				Returns the flux units of the lightcurve.

		"""

		if (zp is None) & (self.zp is not None):
			zp = self.zp
			zp_e = self.zp_e
		elif (zp is None) & (self.zp is None):
			print('Calculating field star zeropoint')
			self.field_calibrate()
			zp = self.zp
			zp_e = self.zp_e

		if flux_type.lower() == 'mjy':
			flux_zp = 16.4
		elif flux_type.lower() == 'jy':
			flux_zp = 8.9
		elif (flux_type.lower() == 'erg') | (flux_type.lower() == 'cgs'):
			flux_zp = -48.6
		elif (flux_type.lower() == 'tess') | (flux_type.lower() == 'counts'):
			if self.tzp is None:
				print('Calculating field star zeropoint')
				self.field_calibrate(plot=plot)
			flux_zp = self.tzp

		else:
			m = '"'+flux_type + '" is not a valid option, please choose from:\njy\nmjy\ncgs/erg\ntess/counts'
			raise ValueError(m)

		flux = self.lc[1] * 10**((zp - flux_zp)/-2.5)
		flux_e2 = ((10**((zp-flux_zp)/-2.5))**2 * self.lc[2]**2 + 
					(self.lc[1]/-2.5 * 10**((zp-flux_zp)/-2.5))**2 * zp_e**2)
		flux_e = np.sqrt(flux_e2)
		self.lc[1] = flux
		self.lc[2] = flux_e


		if flux_type.lower() == 'mjy':
			self.zp = self.zp * 0 + 16.4
			self.zp_e = 0
			self.lc_units = 'mJy'
		if flux_type.lower() == 'jy':
			self.zp = self.zp * 0 + 8.9
			self.zp_e = 0
			self.lc_units = 'Jy'
		elif (flux_type.lower() == 'erg') | (flux_type.lower() == 'cgs'):
			self.zp = self.zp * 0 -48.6
			self.zp_e = 0
			self.lc_units = 'cgs'
		elif (flux_type.lower() == 'tess') | (flux_type.lower() == 'counts'):
			self.zp = self.tzp
			self.zp_e = 0
			self.lc_units = 'Counts'
		return 